{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\pishing'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir:Path\n",
    "    file_path:Path\n",
    "    models_path:Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phishingdetection.constants import *\n",
    "from phishingdetection.utils.common import read_yaml, create_directories\n",
    "from phishingdetection.logging import logger\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "              root_dir=config.root_dir,\n",
    "              file_path= config.file_path,\n",
    "              models_path=config.models_path\n",
    "            \n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phishingdetection.constants import *\n",
    "from phishingdetection.utils.common import read_yaml, create_directories, load_object\n",
    "from phishingdetection.logging import logger\n",
    "import pandas as pd\n",
    "from phishingdetection.logging import logger\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def select_best_model(self):\n",
    "        # Load the model selection data from the file\n",
    "        model_selection_file_path = 'artifacts/model_trainer/model_selection.pkl'\n",
    "        with open(model_selection_file_path, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "\n",
    "        # Check if the loaded data contains the necessary information\n",
    "        if isinstance(loaded_data, tuple) and len(loaded_data) == 2:\n",
    "            model_dataframe, model_info_dict = loaded_data\n",
    "            \n",
    "            # Process DataFrame\n",
    "            best_model = None\n",
    "            best_accuracy = 0\n",
    "            best_precision = None\n",
    "            \n",
    "            for index, row in model_dataframe.iterrows():\n",
    "                accuracy = row['accuracy']\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_model_name = row['models']\n",
    "                    best_precision = row.get('precision', None)  # Precision may not always be present\n",
    "\n",
    "            if best_model_name in model_info_dict:\n",
    "                best_model_info = model_info_dict[best_model_name]\n",
    "                best_model = best_model_info['model']\n",
    "                \n",
    "                logging.info(f\"Best model with accuracy {best_accuracy} and precision {best_precision} loaded successfully.\")\n",
    "                \n",
    "                # Save the best model to another file\n",
    "                best_model_path = 'best_models.pkl'  # Choose a file path for the best model\n",
    "                with open(best_model_path, 'wb') as f:\n",
    "                    pickle.dump(best_model, f)\n",
    "                logging.info(f\"Best model saved to {best_model_path}\")\n",
    "            else:\n",
    "                logging.error(\"No model found with valid accuracy.\")\n",
    "        else:\n",
    "            logging.error(\"Loaded data does not contain the necessary information.\")\n",
    "\n",
    "\n",
    "    def save_models(self, best_model):\n",
    "        logging.info(\"Saving models to file\")\n",
    "        with open(self.config.file_path, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        logging.info(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-09 15:08:36,364: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-04-09 15:08:36,364: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-04-09 15:08:36,370: INFO: common: created directory at: artifacts]\n",
      "[2024-04-09 15:08:36,373: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2024-04-09 15:08:36,534: INFO: 936551666: Best model with accuracy 0.9537507050197406 and precision 0.9537507050197406 loaded successfully.]\n",
      "[2024-04-09 15:08:36,689: INFO: 936551666: Best model saved to best_models.pkl]\n",
      "[2024-04-09 15:08:36,710: INFO: 936551666: Saving models to file]\n",
      "[2024-04-09 15:08:36,715: INFO: 936551666: Models saved successfully.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    best_model=model_evaluation_config.select_best_model()\n",
    "    model_evaluation_config.save_models(best_model)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self,features):\n",
    "        try:\n",
    "            \n",
    "            model=load_object(file_path='artifacts.model_evalution.pkl')\n",
    "            preprocessor=load_object(file_path='scaler.pkl')\n",
    "            print(\"After Loading\")\n",
    "            data_scaled=preprocessor.transform(features)\n",
    "            preds=model.predict(data_scaled)\n",
    "            return preds\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts.model_evalution.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the new data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Preprocess the new data (if needed)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Make sure it has the same format as the data used for training the models\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the models from the saved file\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martifacts.model_evalution.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m     loaded_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     12\u001b[0m     models_dict \u001b[38;5;241m=\u001b[39m loaded_data[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming the models are saved in the second element of the tuple\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'artifacts.model_evalution.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the new data\n",
    "\n",
    "# Preprocess the new data (if needed)\n",
    "# Make sure it has the same format as the data used for training the models\n",
    "\n",
    "# Load the models from the saved file\n",
    "with open('artifacts.model_evalution.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "    models_dict = loaded_data[1]  # Assuming the models are saved in the second element of the tuple\n",
    "\n",
    "# Make predictions using the loaded models\n",
    "\n",
    "    model = models_dict['model']\n",
    "    y_pred = model.predict(X_test)  # Assuming new_data has the same features as X_train\n",
    "    print(f\"Model: {models_dict}, Predictions: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.parse\n",
    "import socket\n",
    "import whois\n",
    "import time\n",
    "\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "    \n",
    "    # URL parsing\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    features['length_url'] = len(url)\n",
    "    features['qty_dot_url'] = url.count('.')\n",
    "    features['qty_hyphen_url'] = url.count('-')\n",
    "    features['qty_underline_url'] = url.count('_')\n",
    "    features['qty_slash_url'] = url.count('/')\n",
    "    features['qty_questionmark_url'] = url.count('?')\n",
    "    features['qty_equal_url'] = url.count('=')\n",
    "    features['qty_at_url'] = url.count('@')\n",
    "    features['qty_and_url'] = url.count('&')\n",
    "    features['qty_exclamation_url'] = url.count('!')\n",
    "    features['qty_space_url'] = url.count(' ')\n",
    "    features['qty_tilde_url'] = url.count('~')\n",
    "    features['qty_comma_url'] = url.count(',')\n",
    "    features['qty_plus_url'] = url.count('+')\n",
    "    features['qty_asterisk_url'] = url.count('*')\n",
    "    features['qty_hashtag_url'] = url.count('#')\n",
    "    features['qty_dollar_url'] = url.count('$')\n",
    "    features['qty_percent_url'] = url.count('%')\n",
    "    \n",
    "    # Domain parsing\n",
    "    domain = parsed_url.netloc\n",
    "    features['qty_dot_domain'] = domain.count('.')\n",
    "    features['qty_hyphen_domain'] = domain.count('-')\n",
    "    features['qty_underline_domain'] = domain.count('_')\n",
    "    features['qty_slash_domain'] = domain.count('/')\n",
    "    features['qty_questionmark_domain'] = domain.count('?')\n",
    "    features['qty_equal_domain'] = domain.count('=')\n",
    "    features['qty_at_domain'] = domain.count('@')\n",
    "    features['qty_and_domain'] = domain.count('&')\n",
    "    features['qty_exclamation_domain'] = domain.count('!')\n",
    "    features['qty_space_domain'] = domain.count(' ')\n",
    "    features['qty_tilde_domain'] = domain.count('~')\n",
    "    features['qty_comma_domain'] = domain.count(',')\n",
    "    features['qty_plus_domain'] = domain.count('+')\n",
    "    features['qty_asterisk_domain'] = domain.count('*')\n",
    "    features['qty_hashtag_domain'] = domain.count('#')\n",
    "    features['qty_dollar_domain'] = domain.count('$')\n",
    "    features['qty_percent_domain'] = domain.count('%')\n",
    "    features['domain_length'] = len(domain)\n",
    "    \n",
    "    # Check if domain resolves to an IP\n",
    "    try:\n",
    "        ip_address = socket.gethostbyname(domain)\n",
    "        features['domain_in_ip'] = 1\n",
    "    except socket.error:\n",
    "        ip_address = None\n",
    "        features['domain_in_ip'] = 0\n",
    "        \n",
    "    # Check if domain is present in URL\n",
    "    features['server_client_domain'] = 1 if domain in url else 0\n",
    "    \n",
    "    # Extract TLD\n",
    "    tld = domain.split('.')[-1]\n",
    "    features['qty_tld_url'] = 1 if tld else 0\n",
    "    \n",
    "    # Extract directory\n",
    "    directory = parsed_url.path\n",
    "    features['qty_dot_directory'] = directory.count('.')\n",
    "    features['qty_hyphen_directory'] = directory.count('-')\n",
    "    features['qty_underline_directory'] = directory.count('_')\n",
    "    features['qty_slash_directory'] = directory.count('/')\n",
    "    features['qty_questionmark_directory'] = directory.count('?')\n",
    "    features['qty_equal_directory'] = directory.count('=')\n",
    "    features['qty_at_directory'] = directory.count('@')\n",
    "    features['qty_and_directory'] = directory.count('&')\n",
    "    features['qty_exclamation_directory'] = directory.count('!')\n",
    "    features['qty_space_directory'] = directory.count(' ')\n",
    "    features['qty_tilde_directory'] = directory.count('~')\n",
    "    features['qty_comma_directory'] = directory.count(',')\n",
    "    features['qty_plus_directory'] = directory.count('+')\n",
    "    features['qty_asterisk_directory'] = directory.count('*')\n",
    "    features['qty_hashtag_directory'] = directory.count('#')\n",
    "    features['qty_dollar_directory'] = directory.count('$')\n",
    "    features['qty_percent_directory'] = directory.count('%')\n",
    "    features['directory_length'] = len(directory)\n",
    "    \n",
    "    # Extract filename\n",
    "    filename = parsed_url.path.split('/')[-1]\n",
    "    features['qty_dot_file'] = filename.count('.')\n",
    "    features['qty_hyphen_file'] = filename.count('-')\n",
    "    features['qty_underline_file'] = filename.count('_')\n",
    "    features['qty_slash_file'] = filename.count('/')\n",
    "    features['qty_questionmark_file'] = filename.count('?')\n",
    "    features['qty_equal_file'] = filename.count('=')\n",
    "    features['qty_at_file'] = filename.count('@')\n",
    "    features['qty_and_file'] = filename.count('&')\n",
    "    features['qty_exclamation_file'] = filename.count('!')\n",
    "    features['qty_space_file'] = filename.count(' ')\n",
    "    features['qty_tilde_file'] = filename.count('~')\n",
    "    features['qty_comma_file'] = filename.count(',')\n",
    "    features['qty_plus_file'] = filename.count('+')\n",
    "    features['qty_asterisk_file'] = filename.count('*')\n",
    "    features['qty_hashtag_file'] = filename.count('#')\n",
    "    features['qty_dollar_file'] = filename.count('$')\n",
    "    features['qty_percent_file'] = filename.count('%')\n",
    "    features['file_length'] = len(filename)\n",
    "    \n",
    "    # Extract parameters\n",
    "    parameters = parsed_url.query\n",
    "    features['qty_dot_params'] = parameters.count('.')\n",
    "    features['qty_hyphen_params'] = parameters.count('-')\n",
    "    features['qty_underline_params'] = parameters.count('_')\n",
    "    features['qty_slash_params'] = parameters.count('/')\n",
    "    features['qty_questionmark_params'] = parameters.count('?')\n",
    "    features['qty_equal_params'] = parameters.count('=')\n",
    "    features['qty_at_params'] = parameters.count('@')\n",
    "    features['qty_and_params'] = parameters.count('&')\n",
    "    features['qty_exclamation_params'] = parameters.count('!')\n",
    "    features['qty_space_params'] = parameters.count(' ')\n",
    "    features['qty_tilde_params'] = parameters.count('~')\n",
    "    features['qty_comma_params'] = parameters.count(',')\n",
    "    features['qty_plus_params'] = parameters.count('+')\n",
    "    features['qty_asterisk_params'] = parameters.count('*')\n",
    "    features['qty_hashtag_params'] = parameters.count('#')\n",
    "    features['qty_dollar_params'] = parameters.count('$')\n",
    "    features['qty_percent_params'] = parameters.count('%')\n",
    "    features['params_length'] = len(parameters)\n",
    "    features['tld_present_params'] = 1 if tld in parameters else 0\n",
    "    \n",
    "    # Check if email present in URL\n",
    "    features['qty_params'] = parameters.count('@')\n",
    "    features['email_in_url'] = 1 if '@' in url else 0\n",
    "    \n",
    "    # Response time\n",
    "    start_time = time.time()\n",
    "    response = urllib.request.urlopen(url)\n",
    "    end_time = time.time()\n",
    "    features['time_response'] = end_time - start_time\n",
    "    \n",
    "    # SPF record of domain\n",
    "    try:\n",
    "        spf_record = whois.whois(domain).get('spf', None)\n",
    "        features['domain_spf'] = 1 if spf_record else 0\n",
    "    except Exception:\n",
    "        features['domain_spf'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=extract_features(\"https://pypi.org/project/python-whois/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded data: <class 'dict'>\n",
      "Loaded data: {'model': RandomForestClassifier(), 'accuracy': 0.9574078912986673, 'precision': 0.9574078912986673}\n",
      "The loaded data does not contain a 'models' dictionary.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open('models.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Step 1: Check the type of loaded data\n",
    "print(\"Type of loaded data:\", type(data))\n",
    "\n",
    "# Step 2: Inspect the loaded data\n",
    "print(\"Loaded data:\", data)\n",
    "\n",
    "# Step 3: Access the 'models' dictionary if present\n",
    "if isinstance(data, dict):\n",
    "    models_dict = data.get('models')\n",
    "    if models_dict is not None:\n",
    "        # Now you can proceed to check the types of values associated with each key in models_dict\n",
    "        for key, value in models_dict.items():\n",
    "            print(\"Key:\", key, \"Value type:\", type(value))\n",
    "    else:\n",
    "        print(\"The loaded data does not contain a 'models' dictionary.\")\n",
    "else:\n",
    "    print(\"The loaded data is not a dictionary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m best_model \u001b[38;5;241m=\u001b[39m models_dict\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Now you can use the best_model for prediction\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# For example, if you have some new data 'X_test', you can predict using:\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('models.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Access the models dictionary\n",
    "models_dict = data['model']\n",
    "\n",
    "# Assuming index 5 contains the trained DecisionTreeClassifier model\n",
    "best_model = models_dict\n",
    "\n",
    "# Now you can use the best_model for prediction\n",
    "# For example, if you have some new data 'X_test', you can predict using:\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25620\\1224121429.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Load the model selection data from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1464\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1467\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import logging\n",
    "\n",
    "# Load the model selection data from the file\n",
    "model_selection_file_path = 'saved_models.pkl'\n",
    "with open(model_selection_file_path, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Check if the loaded data contains the necessary information\n",
    "if 'model' in loaded_data and 'accuracy' in loaded_data:\n",
    "    best_model = loaded_data['model']\n",
    "    best_accuracy = loaded_data['accuracy']\n",
    "    best_precision = loaded_data.get('precision', None)  # Precision may not always be present\n",
    "    logging.info(f\"Best model with accuracy {best_accuracy} and precision {best_precision} loaded successfully.\")\n",
    "\n",
    "    # Save the best model to another file\n",
    "    best_model_path = 'best_model.pkl'  # Choose a file path for the best model\n",
    "    with open(best_model_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    logging.info(f\"Best model saved to {best_model_path}\")\n",
    "else:\n",
    "    logging.error(\"Loaded data does not contain the necessary information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and is an instance of RandomForestClassifier.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier  # Assuming RandomForestClassifier is the model you're using\n",
    "\n",
    "# Load the model from the file\n",
    "model_file_path = 'best_model.pkl'\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Check if the loaded object is an instance of RandomForestClassifier\n",
    "if isinstance(loaded_model, RandomForestClassifier):\n",
    "    print(\"Model loaded successfully and is an instance of RandomForestClassifier.\")\n",
    "else:\n",
    "    print(\"Error: The loaded object is not an instance of RandomForestClassifier.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phishingdetection.utils.common import expected_features, extract_features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from phishingdetection.utils.common import expected_features, extract_features\n",
    "\n",
    "# Load the model\n",
    "model_file_path = 'best_model.pkl'\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load the scaler\n",
    "scaler_file_path = 'scalers.pkl'\n",
    "with open(scaler_file_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Assuming you have a URL for which you want to make a prediction\n",
    "url = \"https://in-a-flask-application\"\n",
    "\n",
    "# Extract features from the URL (assuming this function exists)\n",
    "features = extract_features(url)\n",
    "\n",
    "# Match expected features\n",
    "features = {feature: features.get(feature, 0) for feature in expected_features}\n",
    "\n",
    "# Sort features based on the expected feature order\n",
    "features = [x for _, x in sorted(zip(expected_features, features.values()))]\n",
    "\n",
    "# Reshape features to match the input shape expected by the model\n",
    "feature_values = [features]\n",
    "\n",
    "# Scale features\n",
    "scaled_feature_values = scaler.transform(feature_values)\n",
    "\n",
    "# Predict using the loaded machine learning model\n",
    "prediction = model.predict(scaled_feature_values)\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but MinMaxScaler is expecting 111 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m feature_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(feature_values)\n\u001b[0;32m     28\u001b[0m feature_values \u001b[38;5;241m=\u001b[39m feature_values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m scaled_feature_values \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Predict using the loaded machine learning model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(scaled_feature_values)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:515\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m    Transformed data.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 515\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    524\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1 features, but MinMaxScaler is expecting 111 features as input."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from phishingdetection.utils.common import expected_features, extract_features\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_file_path = 'best_model.pkl'\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load the scaler\n",
    "scaler_file_path = 'scaler.pkl'\n",
    "with open(scaler_file_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Assuming you have a URL for which you want to make a prediction\n",
    "url = \"https://www.digitalocean.com/community/tutorials/how-to-handle-errors-in-a-flask-application\"\n",
    "\n",
    "# Extract features from the URL (assuming this function exists)\n",
    "features = extract_features(url)\n",
    "features = extract_features(url)\n",
    "\n",
    "# Arrange dict_two based on the order of expected_features\n",
    "arranged_dict_two = {feature: features.get(feature, 0) for feature in expected_features}\n",
    "feature_values= [value for value in arranged_dict_two.values()]\n",
    "feature_values = np.array(feature_values)\n",
    "feature_values = feature_values.reshape(-1, 1)\n",
    "\n",
    "scaled_feature_values = scaler.transform(feature_values)\n",
    "\n",
    "# Predict using the loaded machine learning model\n",
    "prediction = model.predict(scaled_feature_values)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "\n",
    "# Print or use arranged_dict_two\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\pishing\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8397352147025521\n",
      "Accuracy: 0.8397352147025521\n"
     ]
    }
   ],
   "source": [
    "from phishingdetection.utils.common import expected_features, extract_features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Load the model from the file\n",
    "model_file_path = 'best_model.pkl'\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    scaler=MinMaxScaler()\n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(r\"artifacts\\data_ingestion\\test.csv\")\n",
    "    x_test = test_data.iloc[:, :-1]  # Features\n",
    "    y_test = test_data.iloc[:, -1]   # Target\n",
    "    x_test= scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "    # Predict using the loaded machine learning model\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11481, 111), (11481,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape ,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and is an instance of RandomForestClassifier.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the model from the file\n",
    "model_file_path = 'best_model.pkl'\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Check if the loaded object is an instance of RandomForestClassifier\n",
    "if isinstance(loaded_model, RandomForestClassifier):\n",
    "    print(\"Model loaded successfully and is an instance of RandomForestClassifier.\")\n",
    "else:\n",
    "    print(\"Error: The loaded object is not an instance of RandomForestClassifier.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching features: 98\n"
     ]
    }
   ],
   "source": [
    "features = extract_features(\"https://www.digitalocean.com/community/tutorials/how-to-handle-errors-in-a-flask-application\")\n",
    "\n",
    "# Count how many expected features are present in the features dictionary\n",
    "matching_count = sum(1 for feature in expected_features if feature in features)\n",
    "\n",
    "print(\"Number of matching features:\", matching_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded data: <class 'dict'>\n",
      "Loaded data: {'model': RandomForestClassifier(), 'accuracy': 0.9574078912986673, 'precision': 0.9574078912986673}\n",
      "The loaded data does not contain a 'models' dictionary.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open('models.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Step 1: Check the type of loaded data\n",
    "print(\"Type of loaded data:\", type(data))\n",
    "\n",
    "# Step 2: Inspect the loaded data\n",
    "print(\"Loaded data:\", data)\n",
    "\n",
    "# Step 3: Access the 'models' dictionary if present\n",
    "if isinstance(data, dict):\n",
    "    models_dict = data.get('models')\n",
    "    if models_dict is not None:\n",
    "        # Now you can proceed to check the types of values associated with each key in models_dict\n",
    "        for key, value in models_dict.items():\n",
    "            print(\"Key:\", key, \"Value type:\", type(value))\n",
    "    else:\n",
    "        print(\"The loaded data does not contain a 'models' dictionary.\")\n",
    "else:\n",
    "    print(\"The loaded data is not a dictionary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qty_dot_file', 'qty_tld_url', 'server_client_domain', 'time_response', 'qty_dot_params', 'qty_dot_domain', 'tld_present_params', 'qty_exclamation_directory', 'qty_exclamation_domain', 'qty_exclamation_file', 'qty_exclamation_params', 'qty_at_url', 'qty_hashtag_directory', 'qty_hashtag_domain', 'qty_hashtag_file', 'qty_hashtag_params', 'qty_plus_url', 'qty_and_directory', 'qty_and_domain', 'qty_and_file', 'qty_and_params', 'qty_equal_url', 'qty_plus_directory', 'qty_plus_domain', 'qty_plus_file', 'qty_plus_params', 'qty_tilde_url', 'qty_percent_directory', 'qty_percent_domain', 'qty_percent_file', 'qty_percent_params', 'qty_hashtag_url', 'qty_hyphen_directory', 'qty_hyphen_domain', 'qty_hyphen_file', 'qty_hyphen_params', 'length_url', 'qty_at_directory', 'qty_at_domain', 'qty_at_file', 'qty_at_params', 'qty_questionmark_url', 'qty_space_directory', 'qty_space_domain', 'qty_space_file', 'qty_space_params', 'qty_and_url', 'qty_dollar_directory', 'qty_dollar_domain', 'qty_dollar_file', 'qty_dollar_params', 'qty_asterisk_url', 'qty_underline_directory', 'qty_underline_domain', 'qty_underline_file', 'qty_underline_params', 'qty_dot_url', 'email_in_url', 'directory_length', 'domain_length', 'file_length', 'params_length', 'qty_dollar_url', 'qty_asterisk_directory', 'qty_asterisk_domain', 'qty_asterisk_file', 'qty_asterisk_params', 'qty_comma_url', 'qty_equal_directory', 'qty_equal_domain', 'qty_equal_file', 'qty_equal_params', 'qty_slash_url', 'qty_questionmark_directory', 'qty_questionmark_domain', 'qty_questionmark_file', 'qty_questionmark_params', 'qty_underline_url', 'qty_tilde_directory', 'qty_tilde_domain', 'qty_tilde_file', 'qty_tilde_params', 'qty_exclamation_url', 'qty_comma_directory', 'qty_comma_domain', 'qty_comma_file', 'qty_comma_params', 'qty_space_url', 'qty_percent_url', 'qty_slash_directory', 'qty_slash_domain', 'qty_slash_file', 'qty_slash_params', 'qty_hyphen_url', 'domain_in_ip', 'qty_dot_directory', 'domain_spf', 'qty_params']\n"
     ]
    }
   ],
   "source": [
    "from phishingdetection.utils.common import expected_features, extract_features\n",
    "extracted_features = extract_features(\"https://www.digitalocean.com/community/tutorials/how-to-handle-errors-in-a-flask-application\")\n",
    "\n",
    "# Combine expected features with extracted features\n",
    "sorted_extracted_features = [x for _, x in sorted(zip(expected_features, extracted_features))]\n",
    "\n",
    "print(sorted_extracted_features)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
